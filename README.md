# ğŸ›°ï¸ GeoExtract-VLM: Satellite Imagery Analysis using Vision Language Models

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Active-brightgreen.svg)

**A Vision Language Model fine-tuned for geospatial analysis and satellite imagery interpretation**

[Features](#features) â€¢ [Installation](#installation) â€¢ [Usage](#usage) â€¢ [Model Architecture](#model-architecture) â€¢ [Roadmap](#roadmap)

</div>

---

## ğŸ¯ Overview

GeoExtract-VLM is a specialized Vision Language Model designed for automated analysis of satellite and aerial imagery. Built on the Qwen2-VL architecture, this model enables natural language querying of geospatial data, making satellite imagery analysis accessible through conversational AI.

### Key Capabilities

- ğŸ¢ **Building Detection & Counting** - Identify and count structures in satellite images
- ğŸ˜ï¸ **Urban Density Assessment** - Analyze population density and urban development patterns
- ğŸ›£ï¸ **Infrastructure Analysis** - Detect roads, transportation networks, and utilities
- ğŸŒ **Land Use Classification** - Categorize areas as residential, commercial, industrial, etc.
- ğŸ“ **Natural Language Interaction** - Ask questions about images in plain English

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| **CLI Interface** | Interactive terminal-based image analysis |
| **GGUF Format** | Optimized for CPU inference, no GPU required |
| **Chain-of-Thought** | Structured reasoning for accurate analysis |
| **Lightweight** | ~1GB model size with 4-bit quantization |
| **Offline Ready** | Works without internet connection |

---

## ğŸš€ Installation

### Prerequisites

- Python 3.10 or higher
- 4GB+ RAM recommended
- ~2GB disk space

### Quick Start

```bash
# Clone the repository
git clone https://github.com/CS-Fasih/GeoExract-Vlm-CLI-Based.git
cd GeoExract-Vlm-CLI-Based

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Run the CLI
python vlm_inference.py
```

### Model Download

The GGUF model file (~940MB) needs to be downloaded separately:
```bash
# Download from releases or contact maintainer
# Place in project root: qwen2vl-satellite-q4_k_m.gguf
```

---

## ğŸ’» Usage

### Interactive CLI Mode

```bash
python vlm_inference.py
```

**Menu Options:**
```
[1] ğŸ“· Load Image & Ask Questions
[2] ğŸ” Quick Auto-Analysis
[3] ğŸ’¬ Text Chat Mode
[4] ğŸ“Š Model Information
[5] ğŸšª Exit
```

### Example Queries

```python
# Building Analysis
"How many buildings can you identify in this satellite image?"

# Urban Density
"Assess the urban density and development pattern in this area"

# Infrastructure
"Identify the road network and transportation infrastructure"

# Land Classification
"What type of land use is predominant in this image?"
```

### Programmatic Usage

```python
from llama_cpp import Llama

# Load model
model = Llama(
    model_path="qwen2vl-satellite-q4_k_m.gguf",
    n_ctx=2048,
    n_threads=4
)

# Analyze
prompt = """<|im_start|>system
You are a geospatial analyst specializing in satellite imagery.<|im_end|>
<|im_start|>user
Describe the urban features in this satellite image.<|im_end|>
<|im_start|>assistant
"""

response = model(prompt, max_tokens=512)
print(response['choices'][0]['text'])
```

---

## ğŸ—ï¸ Model Architecture

| Component | Specification |
|-----------|--------------|
| **Base Model** | Qwen2-VL-2B-Instruct |
| **Fine-tuning Method** | QLoRA (4-bit quantization) |
| **LoRA Rank** | 64 |
| **Training Framework** | Transformers + PEFT |
| **Export Format** | GGUF (llama.cpp compatible) |
| **Quantization** | q4_k_m |
| **Model Size** | ~940MB |

### Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data           â”‚â”€â”€â”€â”€â–¶â”‚  Fine-tuning    â”‚â”€â”€â”€â”€â–¶â”‚  Export         â”‚
â”‚  Acquisition    â”‚     â”‚  with QLoRA     â”‚     â”‚  to GGUF        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â–¼                       â–¼                       â–¼
   SpaceNet               Qwen2-VL-2B              q4_k_m
   Dataset               + LoRA Adapters          Quantized
```

---

## ğŸ“ Project Structure

```
GeoExtract-VLM/
â”œâ”€â”€ ğŸ“„ vlm_inference.py          # Interactive CLI application
â”œâ”€â”€ ğŸ“„ step1_data_acquisition.py # Data download scripts
â”œâ”€â”€ ğŸ“„ step2_preprocessing.py    # Dataset preparation
â”œâ”€â”€ ğŸ“„ step3_finetuning.py       # Model training
â”œâ”€â”€ ğŸ“„ step4_inference.py        # Inference utilities
â”œâ”€â”€ ğŸ“„ step5_export_gguf.py      # GGUF export pipeline
â”œâ”€â”€ ğŸ““ VLM_Satellite_Complete_Pipeline.ipynb  # Full training notebook
â”œâ”€â”€ ğŸ“„ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ“„ .gitignore               # Git ignore rules
â””â”€â”€ ğŸ“„ README.md                # Documentation
```

---

## ğŸ—ºï¸ Roadmap

### Phase 1: CLI Application âœ…
- [x] Fine-tuned VLM model
- [x] Interactive terminal interface
- [x] GGUF export for CPU inference
- [x] Basic image analysis capabilities

### Phase 2: Enhanced Model (In Progress)
- [ ] Expand training dataset (multi-region coverage)
- [ ] Improve building detection accuracy
- [ ] Add segmentation capabilities
- [ ] Multi-language support

### Phase 3: Full-Stack Web Application (Planned)
- [ ] REST API backend (FastAPI/Flask)
- [ ] React/Next.js frontend
- [ ] Map integration (Leaflet/Mapbox)
- [ ] User authentication
- [ ] Batch processing support
- [ ] Cloud deployment (AWS/GCP)

### Phase 4: Advanced Features (Future)
- [ ] Real-time satellite feed analysis
- [ ] Change detection over time
- [ ] Custom region training
- [ ] Mobile application

---

## ğŸ”§ Technical Requirements

### Hardware (Minimum)
- **CPU**: 4+ cores recommended
- **RAM**: 4GB minimum, 8GB recommended
- **Storage**: 2GB free space

### Software
- Python 3.10+
- llama-cpp-python
- Pillow
- rich (for CLI UI)

---

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| Inference Time (CPU) | 20-60 seconds |
| Memory Usage | ~2GB |
| Model Load Time | ~5 seconds |

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Muhammad Fasih**
- GitHub: [@CS-Fasih](https://github.com/CS-Fasih)

---

## ğŸ™ Acknowledgments

- [Qwen2-VL](https://github.com/QwenLM/Qwen2-VL) - Base model architecture
- [SpaceNet](https://spacenet.ai/) - Satellite imagery dataset
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - GGUF format and inference
- [Hugging Face](https://huggingface.co/) - Transformers library

---

<div align="center">

**â­ Star this repository if you find it useful!**

Made with â¤ï¸ for geospatial AI research

</div>
